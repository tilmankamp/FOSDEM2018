<!doctype html>
<html>
	<head>
		<meta charset="utf-8"/>
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"/>

		<title>DeepSpeech</title>

		<link rel="stylesheet" href="css/reveal.css"/>
		<link rel="stylesheet" href="css/theme/black.css"/>
		<link rel="stylesheet" href="css/asciinema-player.css"/>

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css"/>

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h2>DeepSpeech &amp; CommonVoice</h2>
					Tilman Kamp, Jan 2018
				</section>
				<section>
					<img src="images/HAL9000.svg" alt="HAL9000" height="400em" style="border:0; background:transparent; box-shadow: none"/>
				</section>
				<section>
					<h3>User's perspective (embedding)</h3>
				</section>
				<section>
					<asciinema-player font-size="16" src="casts/preparation.cast"></asciinema-player>
					Preparation of the environment
				</section>
				<section>
					<p><a href="audio/8455-210777-0068.wav">8455-210777-0068.wav</a></p>
					<audio controls>
						<source src="audio/8455-210777-0068.wav" type="audio/wav"/>
						Your browser does not support the audio element.
					</audio>
					<p>»Your power is sufficient, I said.«</p>
				</section>
				<section>
					<asciinema-player font-size="16" src="casts/python_usage.cast"></asciinema-player>
					Python usage
				</section>
				<section>
<pre><code class="python" style="font-size: 20px">from deepspeech.model import Model
# ...
def main():
    # ...
    ds = Model(args.model, ..., args.alphabet, ...)
    ds.enableDecoderWithLM(args.alphabet, args.lm, args.trie, ...)
    rate, audio = wav.read(args.audio)
    transcript = ds.stt(audio, rate)
    print(transcript)
if __name__ == '__main__':
	main()</code></pre>
					Relevant Python code
				</section>
				<section>
					<asciinema-player font-size="16" src="casts/js_usage.cast"></asciinema-player>
					Node.js usage
				</section>
								<section>
<pre><code class="javascript" style="font-size: 16px">var audioStream = new MemoryStream()
Fs.createReadStream(process.argv[3]).
    pipe(Sox({ output: { bits: 16, rate: 16000, channels: 1, type: 'raw' } })).
    pipe(audioStream)
audioStream.on('finish', () => {
    audioBuffer = audioStream.toBuffer()
    var model = new Ds.Model(process.argv[2], ..., process.argv[4])
    if (process.argv.length > 6) {
        model.enableDecoderWithLM(process.argv[4], process.argv[5], process.argv[6], ...)
    }
    // LocalDsSTT() expected a short*
    console.log(model.stt(audioBuffer.slice(0, audioBuffer.length / 2), 16000))
});</code></pre>
					Relevant Node.js code
				</section>
				<section>
					But how to train a model?
				</section>
				<section>
					<asciinema-player font-size="16" src="casts/ds_prep.cast"></asciinema-player>
					DeepSpeech training preparation
				</section>
				<section>
					Some words about data
				</section>
				<section>
					<img src="images/cv_page.png" height="500px" alt="voice.mozilla.org"/>
					<br/>
					https://voice.mozilla.org
				</section>
				<section>
					<img src="images/cv_record.png" height="500px" alt="voice.mozilla.org/record"/>
					<br/>
					Contributing voice samples
				</section>
				<section>
					<img src="images/cv_validate.png" width="800px" alt="voice.mozilla.org"/>
					<br/>
					Validating voice samples
				</section>
				<section>
					<img src="images/cv_download.png" height="500px" alt="voice.mozilla.org/data"/>
					<br/>
					https://voice.mozilla.org/data
				</section>
				<section>
					<asciinema-player font-size="16" src="casts/cv_prep.cast"></asciinema-player>
					CommonVoice corpus installation
				</section>
				<section>
					<img src="images/loss_dev.svg" height="500px" alt="Loss development" style="border:0; background:transparent; box-shadow: none"/>
				</section>
				<section data-markdown data-separator="---">
					<script type="text/template">
						# DeepSpeech

						- human voice increasingly important UI
						- current state of the art
							- proprietary
							- user lock-in
							- online-only

						---

						### Introduction
						- A short history of automatic speech recognition (ASR)
						- What is the motivation behind project DeepSpeech?
						- How does DeepSpeech work?
						- How good is DeepSpeech compared to other solutions?

						---

						### Differences to current solutions

						- end-to-end deep learning solution
						- translates raw audio data into text - no domain specific code in between
						- requires huge amount of voice data

						---

						### Using DeepSpeech
						- How to voice-enable a project using DeepSpech
						- How to translate audio data into text
						- Looking into the demo code

						---

						### About data
						- What kind of data is required to train a model?
						- Which corpora are we currently using to train our models?
						- Common Voice: What is it and how to contribute?
						- Unboxing first version of Common Voice corpus

						---

						### Demo
						- How to import (corpus) data
						- How to create a simple corpus from own samples
						- How to augment samples with noise

						---

						## Training a DeepSpeech model
						- How does training work?
						- What are train, dev and test data-sets used for?
						- What are the required software components?
						- What are the hardware requirements?

						---

						### Demo
						- How to train a simple model

						---

						### Roadmap
						- What's our roadmap for 2018+?

						---

						## Q&amp;A
                    </script>
				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
		<script src="js/asciinema-player.js"></script>
	</body>
</html>
