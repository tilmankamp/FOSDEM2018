<!doctype html>
<html>
	<head>
		<meta charset="utf-8"/>
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"/>

		<title>DeepSpeech</title>

		<link rel="stylesheet" href="css/reveal.css"/>
		<link rel="stylesheet" href="css/theme/black.css"/>
		<link rel="stylesheet" href="css/asciinema-player.css"/>

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css"/>

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h2>DeepSpeech &amp; CommonVoice</h2>
					<p>Tilman Kamp, FOSDEM 2018</p>
					<br/>
					<p><strong>tilmankamp.github.io/FOSDEM2018</strong></p>
				</section>
				<section>
					<h3>What are we doing?</h3>
				</section>
				<section>
					<p><strong>https://github.com/mozilla/DeepSpeech</strong></p>
					<p>DeepSpeech is a speech to text (STT) system</p>
					<p>based on machine learning</p>
					<p>(using the TensorFlow machine learning framework)</p>
				</section>
				<section>
					<h3>Why are we doing this?</h3>
				</section>
				<section>
					<h3>Reason 1</h3>
					State of the art alternative to proprietary solutions
				</section>
				<section>
					<img src="images/HAL9000.svg" alt="HAL9000" height="400em" style="border:0; background:transparent; box-shadow: none"/>
					<p>»I am sorry, Dave...«</p>
				</section>
				<section>
					<h3>Reason 2</h3>
					<p>Offline support</p>
					<p>privacy + low latency</p>
				</section>
				<section>
					<h3>Reason 3</h3>
					<p>End-to-end machine learning approach</p>
					<p>scaling + quality</p>
				</section>
				<section>
					<h3>User's perspective (embedding)</h3>
				</section>
				<section>
					<asciinema-player class="stretch" preload="true" font-size="16" src="casts/preparation.cast"></asciinema-player>
					Preparation of the environment
				</section>
				<section>
					<p><a href="audio/8455-210777-0068.wav">8455-210777-0068.wav</a></p>
					<audio controls>
						<source src="audio/8455-210777-0068.wav" type="audio/wav"/>
						Your browser does not support the audio element.
					</audio>
					<p>»Your power is sufficient, I said.«</p>
				</section>
				<section>
					<asciinema-player class="stretch" preload="true" font-size="16" src="casts/python_usage.cast"></asciinema-player>
					Python usage
				</section>
				<section>
<pre><code class="python" style="font-size: 20px">from deepspeech.model import Model
# ...
def main():
    # ...
    ds = Model(args.model, ..., args.alphabet, ...)
    ds.enableDecoderWithLM(args.alphabet, args.lm, args.trie, ...)
    rate, audio = wav.read(args.audio)
    transcript = ds.stt(audio, rate)
    print(transcript)
if __name__ == '__main__':
	main()</code></pre>
					Relevant Python code
				</section>
				<section>
					<asciinema-player class="stretch" preload="true" font-size="16" src="casts/js_usage.cast"></asciinema-player>
					Node.js usage
				</section>
								<section>
<pre><code class="javascript" style="font-size: 16px">var audioStream = new MemoryStream()
Fs.createReadStream(process.argv[3]).
    pipe(Sox({ output: { bits: 16, rate: 16000, channels: 1, type: 'raw' } })).
    pipe(audioStream)
audioStream.on('finish', () => {
    audioBuffer = audioStream.toBuffer()
    var model = new Ds.Model(process.argv[2], ..., process.argv[4])
    if (process.argv.length > 6) {
        model.enableDecoderWithLM(process.argv[4], process.argv[5], process.argv[6], ...)
    }
    // LocalDsSTT() expected a short*
    console.log(model.stt(audioBuffer.slice(0, audioBuffer.length / 2), 16000))
});</code></pre>
					Relevant Node.js code
				</section>
				<section>
					<h2>BTW</h2>
					<p>Our community also provided <strong>Rust &amp; Go</strong> bindings!</p>
				</section>
				<section>
					<h2>Performance</h2>
					<table style="table-layout: fixed">
						<tr><th>Human WER</th><th>DeepSpeech WER</th></tr>
						<tr><td>5.83%</td><td>5.6%</td></tr>
					</table>
					<p>Set: librivox clean test</p>
				</section>
				<section>
					<h2>Training and development</h2>
				</section>
				<section>
					<asciinema-player class="stretch" preload="true" font-size="16" src="casts/ds_prep.cast" speed="2"></asciinema-player>
					DeepSpeech training and development preparation
				</section>
				<section>
					<h2>Getting data</h2>
				</section>
				<section>
					<img src="images/cv_download.png" height="500px" alt="voice.mozilla.org/data"/>
					<p>https://voice.mozilla.org/data</p>
				</section>
				<section>
					<img src="images/cv_page.png" height="500px" alt="voice.mozilla.org"/>
					<p>https://voice.mozilla.org</p>
				</section>
				<section>
					<img src="images/cv_record.png" height="500px" alt="voice.mozilla.org/record"/>
					<p>Contributing voice samples</p>
				</section>
				<section>
					<img src="images/cv_validate.png" width="800px" alt="voice.mozilla.org"/>
					<p>Validating voice samples</p>
				</section>
				<section>
					<asciinema-player class="stretch" preload="true" font-size="16" src="casts/cv_prep.cast"></asciinema-player>
					CommonVoice corpus installation
				</section>
				<section>
					<h2>Illustrating example</h2>
					<p>Training a new language from scratch</p>
				</section>
				<section>
					<p><a href="audio/jakob.wav">jakob.wav</a></p>
					<audio controls>
						<source src="audio/jakob.wav" type="audio/wav"/>
						Your browser does not support the audio element.
					</audio>
					<p><small>»Typograf Jakob zürnt schweißgequält vom öden Text.«</small></p>
				</section>
				<section>
					<asciinema-player class="stretch" preload="true" font-size="16" src="casts/corpus.cast"></asciinema-player>
					Creating a mini corpus from <strong>jakob.wav</strong>
				</section>
				<section>
					<p>&lt;terminology&gt;</p>
				</section>
				<section>
					<p><strong>loss</strong></p>
					<p>how different the actual outcome is from the expected outcome</p>
				</section>
				<section>
					<p><strong>sets in a data corpus</strong></p>
					<table style="table-layout: fixed">
						<tr><td>train-set</td><td>the data the model is trained with</td></tr>
						<tr><td>dev-set</td><td>not trained, but used for validation</td></tr>
						<tr><td>test-set</td><td>unbiased test at the end of the training</td></tr>
					</table>
				</section>
				<section>
					<p><strong>epoch</strong></p>
					<p>the whole train-set got applied to the model one time</p>
				</section>
				<section>
					<img src="images/loss_dev.svg" height="500px" alt="Loss development" style="border:0; background:transparent; box-shadow: none"/>
					<p><strong>overfitting</strong></p>
				</section>
				<section>
					<p><strong>actually we need two models</strong></p>
					<table>
						<tr><th>model</th><th>job</th></tr>
						<tr><td>acoustic</td><td>listens and identifies letters</td></tr>
						<tr><td>language</td><td>reads and understands orthography</td></tr>
					</table>
				</section>
				<section>
					<p>&lt;/terminology&gt;</p>
				</section>
				<section>
					<asciinema-player class="stretch" preload="true" font-size="16" src="casts/lm_prep.cast"></asciinema-player>
					Creating a language model
				</section>
				<section>
					<pre class="stretch"><code class="bash" style="font-size: 20px">#!/usr/bin/env bash
python -u DeepSpeech.py \
    --train_files "/home/demo/miniger/miniger-train.csv" \
    --dev_files "/home/demo/miniger/miniger-train.csv" \
    --test_files "/home/demo/miniger/miniger-train.csv" \
    --alphabet_config_path "/home/demo/german-models/alphabet.txt" \
    --lm_binary_path "/home/demo/german-models/lm.binary" \
    --lm_trie_path "/home/demo/german-models/trie" \
    --learning_rate 0.000025 \
    --dropout_rate 0 \
    --word_count_weight 3.5 \
    --log_level 1 \
    --display_step 1 \
    --epoch 200 \
    --export_dir "/home/demo/german-models"</code></pre>
					Convenience: Preparation of a <strong>.run</strong> script
				</section>
				<section>
					<asciinema-player class="stretch" preload="true" font-size="16" src="casts/run.cast"></asciinema-player>
					Training one sample into extreme overfitting (realtime)
				</section>
				<section>
					<asciinema-player class="stretch" speed="20" preload="true" font-size="16" src="casts/run.cast"></asciinema-player>
					Training one sample into extreme overfitting (20x)
				</section>
				<section>
					<h2>Next steps</h2>
					<p>Get more transcribed voice data</p>
					<p>Get more and contemporary language model texts</p>
					<p>Tune your hyper parameters</p>
					<p>Get hardware - ML is computing-intensive as hell</p>
					<p>Spread the word</p>
				</section>
				<section>
					<h2>Roadmap 2018</h2>
					<p>Another language</p>
					<p>Streaming support</p>
					<p>Optimizing for noisy backgrounds<br/>(Thanks to freesound.org)</p>
					<p>Text To Speech (TTS)</p>
				</section>
				<section>
					<h2>Talk to us</h2>
					<p>https://discourse.mozilla.org/c/deep-speech</p>
					<p>https://wiki.mozilla.org/IRC - #machinelearning</p>
				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				keyboard: {
					86: function(event) {
						var slide = Reveal.getCurrentSlide()
						var player = slide.getElementsByTagName('asciinema-player')[0]
						if(player) {
							if(player.playing) {
								player.pause()
								player.playing = false
							} else {
								player.play()
								player.playing = true
							}
						}
					}
				},
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
		<script src="js/asciinema-player.js"></script>
	</body>
</html>
