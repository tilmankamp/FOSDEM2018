<!doctype html>
<html>
	<head>
		<meta charset="utf-8"/>
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"/>

		<title>DeepSpeech</title>

		<link rel="stylesheet" href="css/reveal.css"/>
		<link rel="stylesheet" href="css/theme/black.css"/>
		<link rel="stylesheet" href="css/asciinema-player.css"/>

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css"/>

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h2>DeepSpeech &amp; CommonVoice</h2>
					Tilman Kamp, Jan 2018
				</section>
				<section>
					<h3>Reason 1</h3>
					State of the Art Alternative to proprietary solutions
				</section>
				<section>
					<img src="images/HAL9000.svg" alt="HAL9000" height="400em" style="border:0; background:transparent; box-shadow: none"/>
				</section>
				<section>
					<h3>Reason 2</h3>
					Offline support to protect privacy and get lower latency
				</section>
				<section>
					<h3>Reason 3</h3>
					End-to-end machine learning approach
				</section>
				<section>
					<h3>User's perspective (embedding)</h3>
				</section>
				<section>
					<asciinema-player font-size="16" src="casts/preparation.cast"></asciinema-player>
					Preparation of the environment
				</section>
				<section>
					<p><a href="audio/8455-210777-0068.wav">8455-210777-0068.wav</a></p>
					<audio controls>
						<source src="audio/8455-210777-0068.wav" type="audio/wav"/>
						Your browser does not support the audio element.
					</audio>
					<p>»Your power is sufficient, I said.«</p>
				</section>
				<section>
					<asciinema-player font-size="16" src="casts/python_usage.cast"></asciinema-player>
					Python usage
				</section>
				<section>
<pre><code class="python" style="font-size: 20px">from deepspeech.model import Model
# ...
def main():
    # ...
    ds = Model(args.model, ..., args.alphabet, ...)
    ds.enableDecoderWithLM(args.alphabet, args.lm, args.trie, ...)
    rate, audio = wav.read(args.audio)
    transcript = ds.stt(audio, rate)
    print(transcript)
if __name__ == '__main__':
	main()</code></pre>
					Relevant Python code
				</section>
				<section>
					<asciinema-player font-size="16" src="casts/js_usage.cast"></asciinema-player>
					Node.js usage
				</section>
								<section>
<pre><code class="javascript" style="font-size: 16px">var audioStream = new MemoryStream()
Fs.createReadStream(process.argv[3]).
    pipe(Sox({ output: { bits: 16, rate: 16000, channels: 1, type: 'raw' } })).
    pipe(audioStream)
audioStream.on('finish', () => {
    audioBuffer = audioStream.toBuffer()
    var model = new Ds.Model(process.argv[2], ..., process.argv[4])
    if (process.argv.length > 6) {
        model.enableDecoderWithLM(process.argv[4], process.argv[5], process.argv[6], ...)
    }
    // LocalDsSTT() expected a short*
    console.log(model.stt(audioBuffer.slice(0, audioBuffer.length / 2), 16000))
});</code></pre>
					Relevant Node.js code
				</section>
				<section>
					<h2>Performance</h2>
					<table style="table-layout: fixed">
						<tr><th>Human WER</th><th>DeepSpeech WER</th></tr>
						<tr><td>5.83%</td><td>5.6%</td></tr>
					</table>
					<p>Set: librivox clean test</p>
				</section>
				<section>
					<h2>Training a model</h2>
				</section>
				<section>
					<asciinema-player font-size="16" src="casts/ds_prep.cast"></asciinema-player>
					DeepSpeech training preparation
				</section>
				<section>
					<h2>Getting data</h2>
				</section>
				<section>
					<img src="images/cv_page.png" height="500px" alt="voice.mozilla.org"/>
					<p>https://voice.mozilla.org</p>
				</section>
				<section>
					<img src="images/cv_record.png" height="500px" alt="voice.mozilla.org/record"/>
					<p>Contributing voice samples</p>
				</section>
				<section>
					<img src="images/cv_validate.png" width="800px" alt="voice.mozilla.org"/>
					<p>Validating voice samples</p>
				</section>
				<section>
					<img src="images/cv_download.png" height="500px" alt="voice.mozilla.org/data"/>
					<p>https://voice.mozilla.org/data</p>
				</section>
				<section>
					<asciinema-player font-size="16" src="casts/cv_prep.cast"></asciinema-player>
					CommonVoice corpus installation
				</section>
				<section>
					<h2>Building a language corpus from scratch</h2>
				</section>
				<section>
					<p><a href="audio/jakob.wav">jakob.wav</a></p>
					<audio controls>
						<source src="audio/jakob.wav" type="audio/wav"/>
						Your browser does not support the audio element.
					</audio>
					<p><small>»Typograf Jakob zürnt schweißgequält vom öden Text.«</small></p>
				</section>
				<section>
					<h2>Some terminology</h2>
				</section>
				<section>
					<img src="images/loss_dev.svg" height="500px" alt="Loss development" style="border:0; background:transparent; box-shadow: none"/>
				</section>
				<section>
<pre><code class="python" style="font-size: 20px">for epoch in range(max_epochs):
    for batch in train_set.get_batches(train_batch_size):
        train_loss = model.compute_loss(batch)
        gradients = optimizer.compute_gradients(train_loss)
        optimizer.apply_gradients(gradients)
    for batch in dev_set.get_batches(dev_batch_size):
        dev_losses.append(model.compute_loss(batch))
    dev_loss = avg(dev_losses)
    if dev_loss > last_dev_loss:
        return # early stop to prevent overfitting
    last_dev_loss = dev_loss
for batch in test_set.get_batches(test_batch_size):
    test_losses.append(model.compute_loss(batch))
test_loss = avg(test_losses)</code></pre>
				</section>
				<section>
					<h2>Roadmap 2018</h2>
					<p>Another language</p>
					<p>Streaming support</p>
					<p>Optimizing for noisy backgrounds<br/>(Thanks to freesound.org)</p>
					<p>Text To Speech (TTS)</p>
				</section>
				<section>
					<h2>Q&amp;A</h2>
				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
		<script src="js/asciinema-player.js"></script>
	</body>
</html>
